{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e978793-1fc7-4dc5-93c3-e13d11b5224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing_JP import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067e3bc6-b043-46c4-91a1-bcb564aea3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_from_preproc(description,df, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    RandomForest w/ / w/o balanced class_weights\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/generated/\n",
    "    sklearn.ensemble.RandomForestClassifier.html\n",
    "    \"\"\"\n",
    "    rf = sklearn.ensemble.RandomForestClassifier(n_jobs = -1,\n",
    "                                                 random_state = 120)\n",
    "    print()\n",
    "    print(description,\": randomforest w/ default parameter values\")\n",
    "    print()\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                               rownames=['real/pred'],\n",
    "                               normalize=True),2).to_markdown())\n",
    "    print()\n",
    "    print(np.round(pd.DataFrame(\n",
    "        sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                           output_dict=True)),2).to_markdown())\n",
    "    print()\n",
    "    #print(pd.DataFrame(\n",
    "    #imblearn.metrics.classification_report_imbalanced(y_test,\n",
    "    #y_pred,\n",
    "    #output_dict=True)).to_markdown())\n",
    "    print(np.round(pd.DataFrame(\n",
    "        rf.feature_importances_,\n",
    "        index=df.columns[:-1],\n",
    "        columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "    print()\n",
    "    \n",
    "    rf = sklearn.ensemble.RandomForestClassifier(n_jobs = -1,\n",
    "                                                 class_weight = \"balanced\",\n",
    "                                                 random_state = 120)\n",
    "    print()\n",
    "    print(description,\": randomforest w/ class_weight=balanced\")\n",
    "    print()\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(np.round(pd.crosstab(y_test,\n",
    "                               y_pred,\n",
    "                               rownames=['real/pred'],\n",
    "                               normalize=True),2).to_markdown())\n",
    "    print()\n",
    "    print(np.round(pd.DataFrame(\n",
    "        sklearn.metrics.classification_report(y_test,\n",
    "                                              y_pred,output_dict=True)),\n",
    "                   2).to_markdown())\n",
    "    print()\n",
    "    #print(pd.DataFrame(imblearn.metrics.classification_report_imbalanced(\n",
    "    #y_test, y_pred,output_dict=True)).to_markdown())\n",
    "    print(np.round(pd.DataFrame(\n",
    "        rf.feature_importances_,\n",
    "        index=df.columns[:-1],\n",
    "        columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "    print()\n",
    "    \n",
    "    rf = sklearn.ensemble.RandomForestClassifier(n_jobs = -1,\n",
    "                                           class_weight = \"balanced_subsample\",\n",
    "                                           random_state = 120)\n",
    "    print()\n",
    "    print(description,\": randomforest w/ class_weight=balanced_subsample\")\n",
    "    print()\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(np.round(pd.crosstab(y_test,\n",
    "                               y_pred,\n",
    "                               rownames=['real/pred'],\n",
    "                               normalize=True),2).to_markdown())\n",
    "    print()\n",
    "    print(np.round(pd.DataFrame(\n",
    "        sklearn.metrics.classification_report(y_test,\n",
    "                                              y_pred,\n",
    "                                              output_dict=True)),\n",
    "                   2).to_markdown())\n",
    "    print()\n",
    "    #print(pd.DataFrame(imblearn.metrics.classification_report_imbalanced(\n",
    "    #y_test, y_pred,output_dict=True)).to_markdown())\n",
    "    print(np.round(pd.DataFrame(\n",
    "        rf.feature_importances_,\n",
    "        index=df.columns[:-1],\n",
    "        columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "    print()\n",
    "    #y_probas = rf.predict_proba(X_test) # P(X_test sample) in  mod/class\n",
    "    # different way of evaluation of perf: \"cumulative lift curve\" / Gain curve\n",
    "    # score for prob of being in target class (rain)\n",
    "    #skplt.metrics.plot_cumulative_gain(y_test, y_probas, figsize=(12,8))\n",
    "    #plt.show() # sorted with decending prob to be in target class\n",
    "    # curve with x axis= percentage of entries 0 to 1 for test set\n",
    "    #  y axis \"gain\" / amount of rainy days\n",
    "\n",
    "    # counteract imbalance: BalancedRandomForestClassifier\n",
    "    # https://imbalanced-learn.org/dev/references/generated/\n",
    "    # imblearn.ensemble.BalancedRandomForestClassifier.html\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "    brf = BalancedRandomForestClassifier(random_state=120)\n",
    "    print()\n",
    "    print(description,\": balancedrandomforest from imblearn\")\n",
    "    print()\n",
    "    brf.fit(X_train, y_train)\n",
    "    y_pred = brf.predict(X_test)\n",
    "    print(np.round(pd.crosstab(y_test,\n",
    "                               y_pred,\n",
    "                               rownames=['real/pred'],\n",
    "                               normalize=True),2).to_markdown())\n",
    "    print()\n",
    "    #print(sklearn.metrics.classification_report(y_test, y_pred))\n",
    "    print(np.round(pd.DataFrame(\n",
    "        sklearn.metrics.classification_report(y_test,\n",
    "                                              y_pred,\n",
    "                                              output_dict=True)),\n",
    "                   2).to_markdown())#,output_dict=True\n",
    "    #print(imblearn.metrics.classification_report_imbalanced(y_test, y_pred))\n",
    "    #print(pd.DataFrame(imblearn.metrics.classification_report_imbalanced(\n",
    "    #y_test, y_pred,output_dict=True)).transpose().to_markdown())\n",
    "    print()\n",
    "    print(np.round(pd.DataFrame(\n",
    "        brf.feature_importances_,\n",
    "        index=df.columns[:-1],\n",
    "        columns=[\"feature_importance\"]).sort_values(\n",
    "                                                by=\"feature_importance\")[-8:],\n",
    "                   2).to_markdown())\n",
    "    print()\n",
    "    #print(pd.DataFrame(brf.feature_importances_,index=df.columns[:-1],\n",
    "    #columns=[\"feature_importance\"]).sort_values(\n",
    "    #by=\"feature_importance\").to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341e79d-aba3-467e-8bb0-ee5a139c1e17",
   "metadata": {},
   "source": [
    "# basic random forest using new (after report2) preprocessing, which includes weighted-average data from nearby stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17616c76-27ad-49e9-8d5b-ba9078074867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100km_NaNrep_local_std_over all : randomforest w/ default parameter values\n",
      "\n",
      "|   real/pred |    0 |    1 |\n",
      "|------------:|-----:|-----:|\n",
      "|           0 | 0.74 | 0.05 |\n",
      "|           1 | 0.1  | 0.11 |\n",
      "\n",
      "|           |        0 |       1 |   accuracy |   macro avg |   weighted avg |\n",
      "|:----------|---------:|--------:|-----------:|------------:|---------------:|\n",
      "| precision |     0.88 |    0.72 |       0.86 |        0.8  |           0.85 |\n",
      "| recall    |     0.94 |    0.54 |       0.86 |        0.74 |           0.86 |\n",
      "| f1-score  |     0.91 |    0.62 |       0.86 |        0.76 |           0.85 |\n",
      "| support   | 10872    | 2930    |       0.86 |    13802    |       13802    |\n",
      "\n",
      "|                   |   feature_importance |\n",
      "|:------------------|---------------------:|\n",
      "| Cloud3pm          |                 0.03 |\n",
      "| avg_Pressure3pm   |                 0.03 |\n",
      "| avg_WindGustSpeed |                 0.03 |\n",
      "| avg_Cloud3pm      |                 0.04 |\n",
      "| Sunshine          |                 0.07 |\n",
      "| avg_Sunshine      |                 0.07 |\n",
      "| Humidity3pm       |                 0.08 |\n",
      "| avg_Humidity3pm   |                 0.09 |\n",
      "\n",
      "save, reload and check model\n",
      "\n",
      "|   real/pred |    0 |    1 |\n",
      "|------------:|-----:|-----:|\n",
      "|           0 | 0.74 | 0.04 |\n",
      "|           1 | 0.1  | 0.12 |\n",
      "\n",
      "|           |        0 |       1 |   accuracy |   macro avg |   weighted avg |\n",
      "|:----------|---------:|--------:|-----------:|------------:|---------------:|\n",
      "| precision |     0.89 |    0.72 |       0.86 |        0.81 |           0.85 |\n",
      "| recall    |     0.94 |    0.55 |       0.86 |        0.75 |           0.86 |\n",
      "| f1-score  |     0.91 |    0.63 |       0.86 |        0.77 |           0.85 |\n",
      "| support   | 10872    | 2930    |       0.86 |    13802    |       13802    |\n",
      "\n",
      "|                   |   feature_importance |\n",
      "|:------------------|---------------------:|\n",
      "| avg_Pressure3pm   |                 0.03 |\n",
      "| Cloud3pm          |                 0.03 |\n",
      "| avg_WindGustSpeed |                 0.03 |\n",
      "| avg_Cloud3pm      |                 0.04 |\n",
      "| Sunshine          |                 0.06 |\n",
      "| avg_Sunshine      |                 0.07 |\n",
      "| avg_Humidity3pm   |                 0.08 |\n",
      "| Humidity3pm       |                 0.08 |\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOUTPUT for alicesprings data:\\n\\nalicesprings_03_nanlocal_std_over all :randomforest w/ default parameter values\\n\\n|   real/pred |    0 |    1 |\\n|------------:|-----:|-----:|\\n|           0 | 0.91 | 0.02 |\\n|           1 | 0.03 | 0.04 |\\n\\n|           |      0 |     1 |   accuracy |   macro avg |   weighted avg |\\n|:----------|-------:|------:|-----------:|------------:|---------------:|\\n| precision |   0.97 |  0.63 |       0.95 |        0.8  |           0.95 |\\n| recall    |   0.98 |  0.58 |       0.95 |        0.78 |           0.95 |\\n| f1-score  |   0.97 |  0.6  |       0.95 |        0.79 |           0.95 |\\n| support   | 480    | 33    |       0.95 |      513    |         513    |\\n\\n|                 |   feature_importance |\\n|:----------------|---------------------:|\\n| avg_Humidity9am |                 0.03 |\\n| avg_Cloud9am    |                 0.04 |\\n| Cloud3pm        |                 0.05 |\\n| avg_Humidity3pm |                 0.06 |\\n| avg_Sunshine    |                 0.08 |\\n| Humidity3pm     |                 0.09 |\\n| Sunshine        |                 0.09 |\\n| avg_Cloud3pm    |                 0.1  |\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for save_output in[\"MelbourneAirport_100km_NaNrep_local_std_over\",\n",
    "                   \"05corr_NaNrep_local_std_over\",\n",
    "                   \"100km_NaNrep_local_std_over\",\n",
    "                   \"300km_NaNrep_local_std_over\",\n",
    "                   \"03corr_NaNrep_local_std_over\"]:\n",
    "    # try to reduce the data to only loc / only loc average features:\n",
    "    for addition in [\"all\",\"only_nonavg\",\"only_avg\"]:\n",
    "        df = pd.read_pickle(\"../data/\"+save_output+\"_df.pkl\")\n",
    "        X_train = pd.read_pickle(\"../data/\"+save_output+\"_Xtrain.pkl\")\n",
    "        X_train\n",
    "        X_test = pd.read_pickle(\"../data/\"+save_output+\"_Xtest.pkl\")\n",
    "        y_train = pd.read_pickle(\"../data/\"+save_output+\"_ytrain.pkl\")\n",
    "        y_test = pd.read_pickle(\"../data/\"+save_output+\"_ytest.pkl\")\n",
    "        \"\"\"\n",
    "        # sort X_train and y_train by Date,\n",
    "        # as the over/undersampling destroyed the sorting\n",
    "        temp = pd.concat([X_train,y_train],axis=1)\n",
    "        temp = temp.sort_values(by=[\"Year\", \"Month\", \"Day\"])\n",
    "        X_train = temp.drop(\"RainTomorrow\",axis=1)\n",
    "        y_train = temp.RainTomorrow\n",
    "        del temp\n",
    "        \"\"\"\n",
    "        if addition == \"all\": pass\n",
    "        else:\n",
    "            locs=[\"Location_Latitude\", \"Location_Longitude\",\n",
    "                \"Location_Elevation\", \"MinTemp\",\"MaxTemp\", \"Evaporation\",\n",
    "                \"Sunshine\", \"WindGustSpeed\", \"WindGustDir_cos\",\n",
    "                \"WindGustDir_sin\", \"WindDir9am_cos\", \"WindDir9am_sin\",\n",
    "                \"WindDir3pm_cos\", \"WindDir3pm_sin\", \"WindSpeed9am\",\n",
    "                \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\",\n",
    "                \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\",\n",
    "                \"Rainfall\", \"RainToday\"]\n",
    "            if addition == \"only_nonavg\":\n",
    "                \n",
    "                X_train = X_train.loc[:,[\"Day\",\"Month\",\"Year\",*locs]]\n",
    "                X_test = X_test.loc[:,[\"Day\",\"Month\",\"Year\",*locs]]\n",
    "                df = df.loc[:,[\"Day\",\"Month\",\"Year\",*locs,\"RainTomorrow\"]]\n",
    "            if addition == \"only_avg\":\n",
    "                locs=[\"avg_\"+loc for loc in locs] # avg\n",
    "                X_train = X_train.loc[:,[\"Day\",\"Month\",\"Year\",*locs]]\n",
    "                X_test = X_test.loc[:,[\"Day\",\"Month\",\"Year\",*locs]]\n",
    "                df = df.loc[:,[\"Day\",\"Month\",\"Year\",*locs,\"RainTomorrow\"]]\n",
    "        # model\n",
    "        rf = sklearn.ensemble.RandomForestClassifier(n_jobs = -1,\n",
    "                                                     random_state = 120)\n",
    "        print()\n",
    "        print(save_output,\n",
    "              addition,\n",
    "              \": randomforest w/ default parameter values\")\n",
    "        print()\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                                   rownames=['real/pred'],\n",
    "                                   normalize=True),2).to_markdown())\n",
    "        print()\n",
    "        print(np.round(pd.DataFrame(\n",
    "            sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                           output_dict=True)),2).to_markdown())\n",
    "        print()\n",
    "        print(np.round(pd.DataFrame(\n",
    "            rf.feature_importances_,\n",
    "            index=df.columns[:-1],\n",
    "            columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "        print()\n",
    "        if ((save_output == \"100km_NaNrep_local_std_over\") and\n",
    "            (addition == \"all\")):\n",
    "            print(\"save, reload and check model\\n\")\n",
    "            #create pkl file with fitted model\n",
    "            filename = '100km_NaNrep_local_std_over_rf_model.pkl'\n",
    "            pickle.dump(rf, open(\"../data/\" + filename, 'wb'))\n",
    "            # load model from pkl file and use it\n",
    "            loaded_model = pickle.load(open(\"../data/\" + filename, 'rb'))\n",
    "            y_pred = loaded_model.predict(X_test)\n",
    "            print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                                   rownames=['real/pred'],\n",
    "                                   normalize=True),2).to_markdown())\n",
    "            print()\n",
    "            print(np.round(pd.DataFrame(\n",
    "                sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                           output_dict=True)),2).to_markdown())\n",
    "            print()\n",
    "            print(np.round(pd.DataFrame(\n",
    "                loaded_model.feature_importances_,\n",
    "                index=df.columns[:-1],\n",
    "                columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "            print()\n",
    "\"\"\"\n",
    "OUTPUT for alicesprings data:\n",
    "\n",
    "alicesprings_03_nanlocal_std_over all :randomforest w/ default parameter values\n",
    "\n",
    "|   real/pred |    0 |    1 |\n",
    "|------------:|-----:|-----:|\n",
    "|           0 | 0.91 | 0.02 |\n",
    "|           1 | 0.03 | 0.04 |\n",
    "\n",
    "|           |      0 |     1 |   accuracy |   macro avg |   weighted avg |\n",
    "|:----------|-------:|------:|-----------:|------------:|---------------:|\n",
    "| precision |   0.97 |  0.63 |       0.95 |        0.8  |           0.95 |\n",
    "| recall    |   0.98 |  0.58 |       0.95 |        0.78 |           0.95 |\n",
    "| f1-score  |   0.97 |  0.6  |       0.95 |        0.79 |           0.95 |\n",
    "| support   | 480    | 33    |       0.95 |      513    |         513    |\n",
    "\n",
    "|                 |   feature_importance |\n",
    "|:----------------|---------------------:|\n",
    "| avg_Humidity9am |                 0.03 |\n",
    "| avg_Cloud9am    |                 0.04 |\n",
    "| Cloud3pm        |                 0.05 |\n",
    "| avg_Humidity3pm |                 0.06 |\n",
    "| avg_Sunshine    |                 0.08 |\n",
    "| Humidity3pm     |                 0.09 |\n",
    "| Sunshine        |                 0.09 |\n",
    "| avg_Cloud3pm    |                 0.1  |\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b757a-dfcb-4b1b-ae5f-6455f01153a9",
   "metadata": {},
   "source": [
    "## testing to improve randomforest and logistic regression with gridsearchcv (hyperparameter optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "76a191a4-e663-4b1c-a3ea-619b114f170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ": randomforest w/ default parameter values\n",
      "\n",
      "|   real/pred |    0 |    1 |\n",
      "|------------:|-----:|-----:|\n",
      "|           0 | 0.72 | 0.06 |\n",
      "|           1 | 0.11 | 0.12 |\n",
      "\n",
      "|           |      0 |      1 |   accuracy |   macro avg |   weighted avg |\n",
      "|:----------|-------:|-------:|-----------:|------------:|---------------:|\n",
      "| precision |   0.87 |   0.68 |       0.84 |        0.78 |           0.83 |\n",
      "| recall    |   0.93 |   0.52 |       0.84 |        0.73 |           0.84 |\n",
      "| f1-score  |   0.9  |   0.59 |       0.84 |        0.74 |           0.83 |\n",
      "| support   | 464    | 134    |       0.84 |      598    |         598    |\n",
      "\n",
      "|                   |   feature_importance |\n",
      "|:------------------|---------------------:|\n",
      "| Pressure9am       |                 0.03 |\n",
      "| avg_WindGustSpeed |                 0.04 |\n",
      "| Pressure3pm       |                 0.04 |\n",
      "| avg_Sunshine      |                 0.05 |\n",
      "| avg_Pressure3pm   |                 0.06 |\n",
      "| Sunshine          |                 0.06 |\n",
      "| avg_Humidity3pm   |                 0.06 |\n",
      "| Humidity3pm       |                 0.07 |\n",
      "\n",
      "\n",
      ": lr w/ default parameter values\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   real/pred |    0 |    1 |\n",
      "|------------:|-----:|-----:|\n",
      "|           0 | 0.72 | 0.06 |\n",
      "|           1 | 0.11 | 0.12 |\n",
      "\n",
      "|           |      0 |      1 |   accuracy |   macro avg |   weighted avg |\n",
      "|:----------|-------:|-------:|-----------:|------------:|---------------:|\n",
      "| precision |   0.87 |   0.68 |       0.84 |        0.78 |           0.83 |\n",
      "| recall    |   0.93 |   0.52 |       0.84 |        0.73 |           0.84 |\n",
      "| f1-score  |   0.9  |   0.59 |       0.84 |        0.74 |           0.83 |\n",
      "| support   | 464    | 134    |       0.84 |      598    |         598    |\n",
      "\n",
      "|                     |   importance |\n",
      "|:--------------------|-------------:|\n",
      "| WindGustSpeed       |         0.48 |\n",
      "| MinTemp             |         0.51 |\n",
      "| Temp9am             |         0.54 |\n",
      "| avg_WindGustDir_cos |         0.59 |\n",
      "| Humidity3pm         |         0.68 |\n",
      "| avg_MaxTemp         |         0.74 |\n",
      "| WindSpeed3pm        |         0.95 |\n",
      "| WindDir9am_cos      |         1    |\n",
      "\n",
      "\n",
      "{'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 10}\n",
      "RandomForestClassifier(criterion='entropy', max_features='log2',\n",
      "                       n_estimators=10, n_jobs=-1, random_state=120)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.760603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.696733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.736731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.716589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.697546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.695662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.738076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.677169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.687355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.705363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.696789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.696543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.720307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.710794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.704390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.718083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.704398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'sqrt...</td>\n",
       "      <td>0.700431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.795087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.773891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.748902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.768656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.771956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.775801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.789539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.750036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.759725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.762682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.780146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.783977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.757947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.747275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.727565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.758361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.765155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'criterion': 'entropy', 'max_features': 'log2...</td>\n",
       "      <td>0.765311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.738091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.652509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.668923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.652529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.627442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.626889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.680669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.633198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.604144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.612427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.624978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.625675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.614236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.661795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.643547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.677229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>0.686478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.758648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.768374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.772502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.780161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.778965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.769498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.755791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.701540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.727872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.756358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.772449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.771173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.788029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.764710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.772253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.772638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.775871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'log2', ...</td>\n",
       "      <td>0.775205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_score\n",
       "0   {'criterion': 'entropy', 'max_features': 'sqrt...         0.760603\n",
       "1   {'criterion': 'entropy', 'max_features': 'sqrt...         0.696733\n",
       "2   {'criterion': 'entropy', 'max_features': 'sqrt...         0.736731\n",
       "3   {'criterion': 'entropy', 'max_features': 'sqrt...         0.716589\n",
       "4   {'criterion': 'entropy', 'max_features': 'sqrt...         0.697546\n",
       "5   {'criterion': 'entropy', 'max_features': 'sqrt...         0.695662\n",
       "6   {'criterion': 'entropy', 'max_features': 'sqrt...         0.738076\n",
       "7   {'criterion': 'entropy', 'max_features': 'sqrt...         0.677169\n",
       "8   {'criterion': 'entropy', 'max_features': 'sqrt...         0.687355\n",
       "9   {'criterion': 'entropy', 'max_features': 'sqrt...         0.705363\n",
       "10  {'criterion': 'entropy', 'max_features': 'sqrt...         0.696789\n",
       "11  {'criterion': 'entropy', 'max_features': 'sqrt...         0.696543\n",
       "12  {'criterion': 'entropy', 'max_features': 'sqrt...         0.720307\n",
       "13  {'criterion': 'entropy', 'max_features': 'sqrt...         0.710794\n",
       "14  {'criterion': 'entropy', 'max_features': 'sqrt...         0.704390\n",
       "15  {'criterion': 'entropy', 'max_features': 'sqrt...         0.718083\n",
       "16  {'criterion': 'entropy', 'max_features': 'sqrt...         0.704398\n",
       "17  {'criterion': 'entropy', 'max_features': 'sqrt...         0.700431\n",
       "18  {'criterion': 'entropy', 'max_features': 'log2...         0.795087\n",
       "19  {'criterion': 'entropy', 'max_features': 'log2...         0.773891\n",
       "20  {'criterion': 'entropy', 'max_features': 'log2...         0.748902\n",
       "21  {'criterion': 'entropy', 'max_features': 'log2...         0.768656\n",
       "22  {'criterion': 'entropy', 'max_features': 'log2...         0.771956\n",
       "23  {'criterion': 'entropy', 'max_features': 'log2...         0.775801\n",
       "24  {'criterion': 'entropy', 'max_features': 'log2...         0.789539\n",
       "25  {'criterion': 'entropy', 'max_features': 'log2...         0.750036\n",
       "26  {'criterion': 'entropy', 'max_features': 'log2...         0.759725\n",
       "27  {'criterion': 'entropy', 'max_features': 'log2...         0.762682\n",
       "28  {'criterion': 'entropy', 'max_features': 'log2...         0.780146\n",
       "29  {'criterion': 'entropy', 'max_features': 'log2...         0.783977\n",
       "30  {'criterion': 'entropy', 'max_features': 'log2...         0.757947\n",
       "31  {'criterion': 'entropy', 'max_features': 'log2...         0.747275\n",
       "32  {'criterion': 'entropy', 'max_features': 'log2...         0.727565\n",
       "33  {'criterion': 'entropy', 'max_features': 'log2...         0.758361\n",
       "34  {'criterion': 'entropy', 'max_features': 'log2...         0.765155\n",
       "35  {'criterion': 'entropy', 'max_features': 'log2...         0.765311\n",
       "36  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.738091\n",
       "37  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.652509\n",
       "38  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.668923\n",
       "39  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.652529\n",
       "40  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.627442\n",
       "41  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.626889\n",
       "42  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.680669\n",
       "43  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.633198\n",
       "44  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.604144\n",
       "45  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.612427\n",
       "46  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.624978\n",
       "47  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.625675\n",
       "48  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.614236\n",
       "49  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.661795\n",
       "50  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.643547\n",
       "51  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.677229\n",
       "52  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.677400\n",
       "53  {'criterion': 'gini', 'max_features': 'sqrt', ...         0.686478\n",
       "54  {'criterion': 'gini', 'max_features': 'log2', ...         0.758648\n",
       "55  {'criterion': 'gini', 'max_features': 'log2', ...         0.768374\n",
       "56  {'criterion': 'gini', 'max_features': 'log2', ...         0.772502\n",
       "57  {'criterion': 'gini', 'max_features': 'log2', ...         0.780161\n",
       "58  {'criterion': 'gini', 'max_features': 'log2', ...         0.778965\n",
       "59  {'criterion': 'gini', 'max_features': 'log2', ...         0.769498\n",
       "60  {'criterion': 'gini', 'max_features': 'log2', ...         0.755791\n",
       "61  {'criterion': 'gini', 'max_features': 'log2', ...         0.701540\n",
       "62  {'criterion': 'gini', 'max_features': 'log2', ...         0.727872\n",
       "63  {'criterion': 'gini', 'max_features': 'log2', ...         0.756358\n",
       "64  {'criterion': 'gini', 'max_features': 'log2', ...         0.772449\n",
       "65  {'criterion': 'gini', 'max_features': 'log2', ...         0.771173\n",
       "66  {'criterion': 'gini', 'max_features': 'log2', ...         0.788029\n",
       "67  {'criterion': 'gini', 'max_features': 'log2', ...         0.764710\n",
       "68  {'criterion': 'gini', 'max_features': 'log2', ...         0.772253\n",
       "69  {'criterion': 'gini', 'max_features': 'log2', ...         0.772638\n",
       "70  {'criterion': 'gini', 'max_features': 'log2', ...         0.775871\n",
       "71  {'criterion': 'gini', 'max_features': 'log2', ...         0.775205"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   real/pred |    0 |    1 |\n",
      "|------------:|-----:|-----:|\n",
      "|           0 | 0.71 | 0.06 |\n",
      "|           1 | 0.11 | 0.11 |\n",
      "\n",
      "|           |      0 |      1 |   accuracy |   macro avg |   weighted avg |\n",
      "|:----------|-------:|-------:|-----------:|------------:|---------------:|\n",
      "| precision |   0.87 |   0.64 |       0.83 |        0.75 |           0.82 |\n",
      "| recall    |   0.92 |   0.51 |       0.83 |        0.71 |           0.83 |\n",
      "| f1-score  |   0.89 |   0.57 |       0.83 |        0.73 |           0.82 |\n",
      "| support   | 464    | 134    |       0.83 |      598    |         598    |\n",
      "\n",
      "|                 |   feature_importance |\n",
      "|:----------------|---------------------:|\n",
      "| avg_Sunshine    |                 0.03 |\n",
      "| Pressure3pm     |                 0.03 |\n",
      "| Pressure9am     |                 0.04 |\n",
      "| avg_Pressure9am |                 0.04 |\n",
      "| Sunshine        |                 0.04 |\n",
      "| avg_Pressure3pm |                 0.06 |\n",
      "| Humidity3pm     |                 0.06 |\n",
      "| avg_Humidity3pm |                 0.06 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n",
      "/home/drs/anaconda3/envs/ds_project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.0001, 'max_iter': 100, 'solver': 'lbfgs'}\n",
      "LogisticRegression(C=0.0001, n_jobs=-1, random_state=120)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 0.0001, 'max_iter': 100, 'solver': 'libl...</td>\n",
       "      <td>0.722923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.0001, 'max_iter': 100, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.737907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 0.0001, 'max_iter': 1000, 'solver': 'lib...</td>\n",
       "      <td>0.722923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 0.0001, 'max_iter': 1000, 'solver': 'lbf...</td>\n",
       "      <td>0.737907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 0.001, 'max_iter': 100, 'solver': 'libli...</td>\n",
       "      <td>0.721325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 0.001, 'max_iter': 100, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.736901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'C': 0.001, 'max_iter': 1000, 'solver': 'libl...</td>\n",
       "      <td>0.721325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'C': 0.001, 'max_iter': 1000, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.736901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'C': 0.01, 'max_iter': 100, 'solver': 'liblin...</td>\n",
       "      <td>0.703787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'C': 0.01, 'max_iter': 100, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.712316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'C': 0.01, 'max_iter': 1000, 'solver': 'libli...</td>\n",
       "      <td>0.703787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'C': 0.01, 'max_iter': 1000, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.712316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'C': 0.1, 'max_iter': 100, 'solver': 'libline...</td>\n",
       "      <td>0.699422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'C': 0.1, 'max_iter': 100, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.701574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'C': 0.1, 'max_iter': 1000, 'solver': 'liblin...</td>\n",
       "      <td>0.699422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'C': 0.1, 'max_iter': 1000, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.701574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'C': 1.0, 'max_iter': 100, 'solver': 'libline...</td>\n",
       "      <td>0.710108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'C': 1.0, 'max_iter': 100, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.710108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'C': 1.0, 'max_iter': 1000, 'solver': 'liblin...</td>\n",
       "      <td>0.710108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'C': 1.0, 'max_iter': 1000, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.710411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'C': 10.0, 'max_iter': 100, 'solver': 'liblin...</td>\n",
       "      <td>0.709234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'C': 10.0, 'max_iter': 100, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'C': 10.0, 'max_iter': 1000, 'solver': 'libli...</td>\n",
       "      <td>0.709234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'C': 10.0, 'max_iter': 1000, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.709234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'C': 100.0, 'max_iter': 100, 'solver': 'libli...</td>\n",
       "      <td>0.711359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'C': 100.0, 'max_iter': 100, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.710479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'C': 100.0, 'max_iter': 1000, 'solver': 'libl...</td>\n",
       "      <td>0.711359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'C': 100.0, 'max_iter': 1000, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.711646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_score\n",
       "0   {'C': 0.0001, 'max_iter': 100, 'solver': 'libl...         0.722923\n",
       "1   {'C': 0.0001, 'max_iter': 100, 'solver': 'lbfgs'}         0.737907\n",
       "2   {'C': 0.0001, 'max_iter': 1000, 'solver': 'lib...         0.722923\n",
       "3   {'C': 0.0001, 'max_iter': 1000, 'solver': 'lbf...         0.737907\n",
       "4   {'C': 0.001, 'max_iter': 100, 'solver': 'libli...         0.721325\n",
       "5    {'C': 0.001, 'max_iter': 100, 'solver': 'lbfgs'}         0.736901\n",
       "6   {'C': 0.001, 'max_iter': 1000, 'solver': 'libl...         0.721325\n",
       "7   {'C': 0.001, 'max_iter': 1000, 'solver': 'lbfgs'}         0.736901\n",
       "8   {'C': 0.01, 'max_iter': 100, 'solver': 'liblin...         0.703787\n",
       "9     {'C': 0.01, 'max_iter': 100, 'solver': 'lbfgs'}         0.712316\n",
       "10  {'C': 0.01, 'max_iter': 1000, 'solver': 'libli...         0.703787\n",
       "11   {'C': 0.01, 'max_iter': 1000, 'solver': 'lbfgs'}         0.712316\n",
       "12  {'C': 0.1, 'max_iter': 100, 'solver': 'libline...         0.699422\n",
       "13     {'C': 0.1, 'max_iter': 100, 'solver': 'lbfgs'}         0.701574\n",
       "14  {'C': 0.1, 'max_iter': 1000, 'solver': 'liblin...         0.699422\n",
       "15    {'C': 0.1, 'max_iter': 1000, 'solver': 'lbfgs'}         0.701574\n",
       "16  {'C': 1.0, 'max_iter': 100, 'solver': 'libline...         0.710108\n",
       "17     {'C': 1.0, 'max_iter': 100, 'solver': 'lbfgs'}         0.710108\n",
       "18  {'C': 1.0, 'max_iter': 1000, 'solver': 'liblin...         0.710108\n",
       "19    {'C': 1.0, 'max_iter': 1000, 'solver': 'lbfgs'}         0.710411\n",
       "20  {'C': 10.0, 'max_iter': 100, 'solver': 'liblin...         0.709234\n",
       "21    {'C': 10.0, 'max_iter': 100, 'solver': 'lbfgs'}         0.707400\n",
       "22  {'C': 10.0, 'max_iter': 1000, 'solver': 'libli...         0.709234\n",
       "23   {'C': 10.0, 'max_iter': 1000, 'solver': 'lbfgs'}         0.709234\n",
       "24  {'C': 100.0, 'max_iter': 100, 'solver': 'libli...         0.711359\n",
       "25   {'C': 100.0, 'max_iter': 100, 'solver': 'lbfgs'}         0.710479\n",
       "26  {'C': 100.0, 'max_iter': 1000, 'solver': 'libl...         0.711359\n",
       "27  {'C': 100.0, 'max_iter': 1000, 'solver': 'lbfgs'}         0.711646"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   real/pred |    0 |    1 |\n",
      "|------------:|-----:|-----:|\n",
      "|           0 | 0.63 | 0.15 |\n",
      "|           1 | 0.06 | 0.16 |\n",
      "\n",
      "|           |      0 |      1 |   accuracy |   macro avg |   weighted avg |\n",
      "|:----------|-------:|-------:|-----------:|------------:|---------------:|\n",
      "| precision |   0.91 |   0.53 |       0.79 |        0.72 |           0.83 |\n",
      "| recall    |   0.81 |   0.73 |       0.79 |        0.77 |           0.79 |\n",
      "| f1-score  |   0.86 |   0.61 |       0.79 |        0.74 |           0.8  |\n",
      "| support   | 464    | 134    |       0.79 |      598    |         598    |\n",
      "\n",
      "|                       |   importance |\n",
      "|:----------------------|-------------:|\n",
      "| avg_WindDir9am_cos    |         0.8  |\n",
      "| RainToday             |         0.8  |\n",
      "| Cloud9am              |         0.85 |\n",
      "| avg_Cloud9am          |         0.85 |\n",
      "| avg_Location_Latitude |         0.94 |\n",
      "| WindGustDir_cos       |         0.94 |\n",
      "| Temp3pm               |         0.99 |\n",
      "| avg_Temp3pm           |         1    |\n"
     ]
    }
   ],
   "source": [
    "save_output=\"MelbourneAirport_100km_NaNrep_local_std_over\"\n",
    "# \"05corr_NaNrep_local_std_over\"\n",
    "# \"100km_NaNrep_local_std_over\"\n",
    "# \"300km_NaNrep_local_std_over\"\n",
    "# \"03corr_NaNrep_local_std_over\"\n",
    "# \"MelbourneAirport_100km_NaNrep_local_std_over\"\n",
    "df = pd.read_pickle(\"../data/\"+save_output+\"_df.pkl\")\n",
    "X_train = pd.read_pickle(\"../data/\"+save_output+\"_Xtrain.pkl\")\n",
    "X_train\n",
    "X_test = pd.read_pickle(\"../data/\"+save_output+\"_Xtest.pkl\")\n",
    "y_train = pd.read_pickle(\"../data/\"+save_output+\"_ytrain.pkl\")\n",
    "y_test = pd.read_pickle(\"../data/\"+save_output+\"_ytest.pkl\")\n",
    "\n",
    "# try to reduce the data to only loc / only loc average features:\n",
    "\"\"\"\n",
    "locs=[\"Location_Latitude\", \"Location_Longitude\",\n",
    "            \"Location_Elevation\", \"MinTemp\",\"MaxTemp\", \"Evaporation\",\n",
    "            \"Sunshine\", \"WindGustSpeed\", \"WindGustDir_cos\", \"WindGustDir_sin\",\n",
    "            \"WindDir9am_cos\", \"WindDir9am_sin\", \"WindDir3pm_cos\",\n",
    "            \"WindDir3pm_sin\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\",\n",
    "            \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\",\n",
    "            \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"Rainfall\", \"RainToday\"]\n",
    "avg_locs=[\"avg_\"+loc for loc in locs] # avg_\n",
    "X_train = X_train.loc[:,[\"Day\",\"Month\",\"Year\",*locs]]\n",
    "X_test = X_test.loc[:,[\"Day\",\"Month\",\"Year\",*locs]]\n",
    "df = df.loc[:,[\"Day\",\"Month\",\"Year\",*locs,\"RainTomorrow\"]]\n",
    "\"\"\"\n",
    "# model\n",
    "\n",
    "###\n",
    "### rf\n",
    "###\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_jobs = -1,\n",
    "                                             random_state = 120)\n",
    "print()\n",
    "print(\": randomforest w/ default parameter values\")\n",
    "print()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                           rownames=['real/pred'],\n",
    "                           normalize=True),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                          output_dict=True)),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    rf.feature_importances_,\n",
    "    index=df.columns[:-1],\n",
    "    columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "print()\n",
    "\n",
    "###\n",
    "### lr\n",
    "###\n",
    "lr = sklearn.linear_model.LogisticRegression(random_state=120, n_jobs = -1)\n",
    "print()\n",
    "print(\": lr w/ default parameter values\")\n",
    "print()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                           rownames=['real/pred'],\n",
    "                           normalize=True),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                          output_dict=True)),2).to_markdown())\n",
    "print()\n",
    "feat_imp = abs(lr.coef_[0])\n",
    "feat_imp =feat_imp / feat_imp.max()\n",
    "idx = np.argsort(feat_imp)\n",
    "feats = np.array(X_test.columns)[idx]\n",
    "print(np.round(pd.DataFrame(\n",
    "    {\"importance\" : feat_imp[idx][-8:]}, index = feats[idx][-8:]),\n",
    "               2).to_markdown())\n",
    "print()\n",
    "print()\n",
    "\n",
    "###\n",
    "### gridsearch on rf\n",
    "###\n",
    "# scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score,pos_label=1)\n",
    "param_grid_rf = [{'n_estimators': [10, 50, 100, 250, 500, 1000],\n",
    "                  'criterion' : [\"entropy\", \"gini\"],\n",
    "                  'min_samples_leaf': [1, 3, 5],\n",
    "                  'max_features': ['sqrt', 'log2']}]\n",
    "best_rf = sklearn.model_selection.GridSearchCV(rf,\n",
    "                                               param_grid_rf,\n",
    "                                               scoring='f1_macro', # or \"f1\"\n",
    "                                               cv=3, n_jobs=-1)\n",
    "best_rf = best_rf.fit(X_train, y_train)\n",
    "print(best_rf.best_params_)\n",
    "print(best_rf.best_estimator_)\n",
    "y_pred = best_rf.predict(X_test)\n",
    "display(pd.DataFrame.from_dict(best_rf.cv_results_).loc[:,\n",
    "                                                ['params', 'mean_test_score']])\n",
    "print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                           rownames=['real/pred'],\n",
    "                           normalize=True),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                          output_dict=True)),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    best_rf.best_estimator_.feature_importances_,\n",
    "    index=df.columns[:-1],\n",
    "    columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "print()\n",
    "\n",
    "###\n",
    "### gridsearch on lr\n",
    "###\n",
    "param_grid_lr = {'solver': ['liblinear', 'lbfgs'],\n",
    "                 'C': np.logspace(-4, 2, 7),\n",
    "                 'max_iter' : [100,1000]}\n",
    "best_lr = sklearn.model_selection.GridSearchCV(lr,\n",
    "                                               param_grid_lr,\n",
    "                                               scoring='f1_macro', # or \"f1\"\n",
    "                                               cv=3, n_jobs=-1)\n",
    "best_lr = best_lr.fit(X_train, y_train)\n",
    "print(best_lr.best_params_)\n",
    "print(best_lr.best_estimator_)\n",
    "y_pred = best_lr.predict(X_test)\n",
    "display(pd.DataFrame.from_dict(best_lr.cv_results_).loc[:,\n",
    "                                                ['params', 'mean_test_score']])\n",
    "print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                           rownames=['real/pred'],\n",
    "                           normalize=True),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                          output_dict=True)),2).to_markdown())\n",
    "print()\n",
    "feat_imp = abs(best_lr.best_estimator_.coef_[0])\n",
    "feat_imp =feat_imp / feat_imp.max()\n",
    "idx = np.argsort(feat_imp)\n",
    "feats = np.array(X_test.columns)[idx]\n",
    "print(np.round(pd.DataFrame(\n",
    "    {\"importance\" : feat_imp[idx][-8:]}, index = feats[idx][-8:]),\n",
    "               2).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad115a-5f61-4d46-a844-fc6d5c43621b",
   "metadata": {},
   "source": [
    "# basic random forest using only two most important features\n",
    "## (temporarily changed the preprocessing function to make this possible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2f7b8-0ba9-410e-83a3-7c53a731ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/weatherAUS.csv\")\n",
    "df, X_train, X_test, y_train, y_test = data_preprocessing(df,\n",
    "                                        nan_treatment_features = \"drop\",\n",
    "                                        sampling = \"over\")\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_jobs = -1,\n",
    "                                             random_state = 120)\n",
    "print()\n",
    "print(\": randomforest w/ default parameter values\")\n",
    "print()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                           rownames=['real/pred'],\n",
    "                           normalize=True),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                          output_dict=True)),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    rf.feature_importances_,\n",
    "    index=df.columns[:-1],\n",
    "    columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "print()\n",
    "\"\"\"\n",
    "OUTPUT:\n",
    "'final data shape (after treating NaNs in features):'\n",
    "\n",
    "(81650, 3)\n",
    "\n",
    "\n",
    ": randomforest w/ default parameter values\n",
    "\n",
    "|   real/pred |    0 |    1 |\n",
    "|------------:|-----:|-----:|\n",
    "|           0 | 0.58 | 0.2  |\n",
    "|           1 | 0.08 | 0.14 |\n",
    "\n",
    "|           |       0 |       1 |   accuracy |   macro avg |   weighted avg |\n",
    "|:----------|--------:|--------:|-----------:|------------:|---------------:|\n",
    "| precision |    0.88 |    0.41 |       0.72 |        0.65 |           0.78 |\n",
    "| recall    |    0.74 |    0.64 |       0.72 |        0.69 |           0.72 |\n",
    "| f1-score  |    0.81 |    0.5  |       0.72 |        0.65 |           0.74 |\n",
    "| support   | 8810    | 2474    |       0.72 |    11284    |       11284    |\n",
    "\n",
    "|             |   feature_importance |\n",
    "|:------------|---------------------:|\n",
    "| Humidity3pm |                 0.46 |\n",
    "| Sunshine    |                 0.54 |\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9199948-54cc-42a9-9e5b-7b850e51261c",
   "metadata": {},
   "source": [
    "# basic random forest using all features\n",
    "## (same preprocessing function as above, but w/o temporal changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c338727-fee5-4a5f-9175-36acee2d8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/weatherAUS.csv\")\n",
    "df, X_train, X_test, y_train, y_test = data_preprocessing(df,\n",
    "                                        nan_treatment_features = \"drop\",\n",
    "                                        sampling = \"over\")\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_jobs = -1,\n",
    "                                             random_state = 120)\n",
    "print()\n",
    "print(\": randomforest w/ default parameter values\")\n",
    "print()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(np.round(pd.crosstab(y_test, y_pred,\n",
    "                           rownames=['real/pred'],\n",
    "                           normalize=True),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    sklearn.metrics.classification_report(y_test, y_pred,\n",
    "                                          output_dict=True)),2).to_markdown())\n",
    "print()\n",
    "print(np.round(pd.DataFrame(\n",
    "    rf.feature_importances_,\n",
    "    index=df.columns[:-1],\n",
    "    columns=[\"feature_importance\"]).sort_values(\n",
    "                                by=\"feature_importance\")[-8:],2).to_markdown())\n",
    "print()\n",
    "\"\"\"\n",
    "OUTPUT:\n",
    "\n",
    "'final data shape (after treating NaNs in features):'\n",
    "\n",
    "(81650, 30)\n",
    "\n",
    "\n",
    ": randomforest w/ default parameter values\n",
    "\n",
    "|   real/pred |    0 |    1 |\n",
    "|------------:|-----:|-----:|\n",
    "|           0 | 0.74 | 0.05 |\n",
    "|           1 | 0.1  | 0.12 |\n",
    "\n",
    "|           |       0 |       1 |   accuracy |   macro avg |   weighted avg |\n",
    "|:----------|--------:|--------:|-----------:|------------:|---------------:|\n",
    "| precision |    0.88 |    0.73 |       0.86 |        0.8  |           0.85 |\n",
    "| recall    |    0.94 |    0.55 |       0.86 |        0.74 |           0.86 |\n",
    "| f1-score  |    0.91 |    0.62 |       0.86 |        0.77 |           0.85 |\n",
    "| support   | 8810    | 2474    |       0.86 |    11284    |       11284    |\n",
    "\n",
    "|               |   feature_importance |\n",
    "|:--------------|---------------------:|\n",
    "| Rainfall      |                 0.04 |\n",
    "| Humidity9am   |                 0.04 |\n",
    "| Pressure9am   |                 0.05 |\n",
    "| WindGustSpeed |                 0.05 |\n",
    "| Pressure3pm   |                 0.05 |\n",
    "| Cloud3pm      |                 0.06 |\n",
    "| Humidity3pm   |                 0.13 |\n",
    "| Sunshine      |                 0.14 |\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f86821-baba-4afb-aa5c-f525622aa3d8",
   "metadata": {},
   "source": [
    "# modeling for report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d34589-07b6-4c2a-b4b7-728a67d29cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/weatherAUS.csv\")\n",
    "\n",
    "model_from_preproc(\"default\",*data_preprocessing(df))\n",
    " # (stdscaler, no sampling, mean/mode over whole col with NaN)\n",
    "model_from_preproc(\"NaN_drop\",\n",
    "                   *data_preprocessing(df, nan_treatment_features = \"drop\"))\n",
    "model_from_preproc(\"oversampling\",\n",
    "                   *data_preprocessing(df, sampling = \"over\"))\n",
    "model_from_preproc(\"undersampling\",\n",
    "                   *data_preprocessing(df, sampling = \"under\"))\n",
    "model_from_preproc(\"NaN_drop oversampling\",\n",
    "                   *data_preprocessing(df, nan_treatment_features = \"drop\",\n",
    "                                       sampling = \"over\"))\n",
    "model_from_preproc(\"NaN_drop undersampling\",\n",
    "                   \n",
    "                   *data_preprocessing(df, nan_treatment_features = \"drop\",\n",
    "                                       sampling = \"under\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e8ec8-f076-4287-bb65-ef1c23b40990",
   "metadata": {},
   "source": [
    "# model selection with nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a208489-8d07-44e6-8c19-9c516e9f36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ToDo: add code here! Or find better way to optimize model performance\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928eb0e-c23b-45fe-83a3-0a2afa2acf92",
   "metadata": {},
   "source": [
    "# save model for future application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89024a-e579-479f-8df9-0ac6e1fc9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "#model = RandomForestClassifier()\n",
    "#model.fit(X_train, y_train)\n",
    "#create pkl file with fitted model\n",
    "filename = 'model.pkl'\n",
    "pickle.dump(rf, open(filename, 'wb'))\n",
    "# load model from pkl file and use it\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.predict(X_test)\n",
    "loaded_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21e3eb-0548-4a3f-ae3e-f95d627fd168",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28223959-d161-4b81-9673-4917a22959f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/weatherAUS.csv\")\n",
    "# de = default parameter values of data_preprocessing()\n",
    "#      (stdscaler, no sampling, mean/mode over whole col with NaN)\n",
    "# ov = over-sampling\n",
    "# un = under-sampling\n",
    "# dr = drop all rows with NaN\n",
    "df_de, X_train_de, X_test_de, y_train_de, y_test_de = data_preprocessing(df)\n",
    "df_dr, X_train_dr, X_test_dr, y_train_dr, y_test_dr = \\\n",
    "    data_preprocessing(df, nan_treatment_features = \"drop\")\n",
    "df_ov, X_train_ov, X_test_ov, y_train_ov, y_test_ov = \\\n",
    "    data_preprocessing(df, sampling = \"over\")\n",
    "df_un, X_train_un, X_test_un, y_train_un, y_test_un = \\\n",
    "    data_preprocessing(df, sampling = \"under\")\n",
    "df_drov, X_train_drov, X_test_drov, y_train_drov, y_test_drov = \\\n",
    "    data_preprocessing(df, nan_treatment_features = \"drop\", sampling = \"over\")\n",
    "df_drun, X_train_drun, X_test_drun, y_train_drun, y_test_drun = \\\n",
    "    data_preprocessing(df, nan_treatment_features = \"drop\", sampling = \"under\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# check if randomover/undersampling does not destroy the temporal\n",
    "# order of train and test set\n",
    "\n",
    "print(pd.Series(y_train_de).value_counts())\n",
    "print(pd.Series(y_train_ov).value_counts())\n",
    "print(pd.Series(y_train_un).value_counts())\n",
    "print(pd.Series(y_train_drov).value_counts())\n",
    "\n",
    "# check if sampler always uses same rows and\n",
    "# does not e.g. mix up Location longs&lats\n",
    "display(df_de[df_de.Location_Latitude==35.31]) \n",
    "display(df_ov[df_ov.Location_Latitude==35.31])\n",
    "display(df_un[df_un.Location_Latitude==35.31])\n",
    "\n",
    "# check if sampler does not generate new dates extending to temporal range\n",
    "# of the test set\n",
    "\n",
    "dates_de = pd.to_datetime(dict(year=X_train_de.Year,\n",
    "                               month=X_train_de.Month,\n",
    "                               day=X_train_de.Day))\n",
    "dates_un = pd.to_datetime(dict(year=X_train_un.Year,\n",
    "                               month=X_train_un.Month,\n",
    "                               day=X_train_un.Day))\n",
    "dates_ov = pd.to_datetime(dict(year=X_train_ov.Year,\n",
    "                               month=X_train_ov.Month,\n",
    "                               day=X_train_ov.Day))\n",
    "print(len(dates_de),len(dates_ov),len(dates_un))\n",
    "# check if sampler keeps same feature value ranges\n",
    "print(X_train_de.min(),X_train_de.max()) \n",
    "print(X_train_ov.min(),X_train_ov.max())\n",
    "print(X_train_un.min(),X_train_un.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342609d-d78f-485e-8848-e373bd1a189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df=pd.read_csv(\"../data/weatherAUS.csv\")\n",
    "print(\n",
    "     np.round(df.groupby(\"Location\").RainTomorrow.value_counts(normalize=True),\n",
    "         2).sort_values())\n",
    "print(\"modeling only the Location 'AliceSprings' yields high accuracy,\\\n",
    " just because it is one of the driest locations in Australia (only 8% raining\\\n",
    " days.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascientest project venv (ds_project)",
   "language": "python",
   "name": "ds_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
